## SOTA 模型

SOTA模型是指当前领域内的最佳模型，全称为"State-of-the-Art"（即技术水平最先进的）模型。它代表了在某个特定任务或领域中目前取得的最好性能。

SOTA模型通常是通过比较不同研究论文、竞赛结果或实验数据来确定的。当一个新模型在特定任务上获得更高的准确度、更低的误差率或其他评价指标时，它将被认为超越了之前被广泛接受和使用的基准模型，并成为该任务领域内新的SOTA模型。

对于机器学习和深度学习领域而言，发展迅速且涌现出许多创新方法和架构。随着时间推移，新提出的算法往往会不断改善并超过以前最好的方法，从而成为新一代SOTA模型。

借助SOTA模型，在各种应用场景下可以获得更精确、有效或鲁棒等方面有所突破，并推动相关领域持续发展。因此，关注和理解当前状态下令人瞩目和优秀的SOTA模型对于跟踪科学进展以及设计自己项目中合适的模型非常重要。

## FLOP

FLOP是一个常用的度量单位，表示浮点运算（Floating Point Operations）的数量。它通常被用来衡量计算机或计算模型在执行某个任务时所需的计算量大小。

具体而言，FLOP指的是**一次浮点运算操作的数量**。这种操作可以是加法、减法、乘法或除法等基本数学运算，其中涉及到浮点数（即带有小数部分的数字）。例如，两个浮点数相乘就需要进行一次FLOP。

对于深度学习中使用的神经网络模型，在训练和推理过程中会涉及大量的矩阵乘法和卷积等复杂计算操作。因此，通过统计网络中总共执行了多少次FLOP可以评估其计算复杂性和效率。

实际上，在机器学习领域中经常使用更大规模单位TFLOPs（Tera-Floating Point Operations per Second）来描述每秒钟能够执行十亿亿次浮点运算。这可以作为衡量硬件设备如GPU或TPU速度和处理能力强弱的标准之一。

总结起来，FLOP是指代表浮点运算数量的度量单位，在机器学习领域用于评估任务所需的计算复杂性，并且常见地应用于衡量硬件设备的计算能力。

> 在TensorFlow 2中，可以使用tf.profiler 来估计模型的FLOP（浮点操作）数量。tf.profiler是一个用于分析和优化TensorFlow性能的工具。
>
> 下面是一种估计模型FLOP的常见方法：
>
> 1. 导入所需的库：
> ```python
> import tensorflow as tf
> from tensorflow.python.profiler import profiler_v2 as profiler
> ```
>
> 2. 构建你想要评估FLOP的模型，并编译它：
> ```python
> model = ... # 在此处构建你的模型
> 
> # 编译模型以准备进行推理
> model.compile(...)
> ```
>
> 3. **创建一个Profiler并运行推理过程**：
> ```python
> profiler.start()
> 
> # 运行你的数据通过模型进行推理，例如使用model.predict或者自定义训练循环等。
> ...
> 
> profiler.stop()
> ```
>
> 4. **生成报告并获取FLOP统计信息**：
> ```python
> profile_result = profiler.profile(
>     tf.get_default_graph(),
>     options=profiler.ProfileOptionBuilder.float_operation(), 
> )
> 
> flop_stats = profile_result.total_float_ops
> 
> print("Total FLOPs: ", flop_stats)
> ```
>
> 这将为您提供总共执行的浮点操作数（FLOPs）。请注意，这个数字表示了整个推理过程期间执行的所有浮点操作数量。
>
> 需要注意以下几点：
> - 在步骤3中，确保在运行前启动Profiler，并在完成后停止它。
> - 步骤4中会打印出总体FLOPs数量。
> - 请确保在安装了TensorFlow 2的环境中运行上述代码。
>
> 这是使用TensorFlow 2来估计模型FLOP的简单示例。通过分析模型的FLOP，您可以更好地了解和优化模型性能，并进行比较和选择不同架构或配置的模型。

## 饱和性质

饱和性质的激活函数是指在**输入数据较大或较小时，激活函数的导数趋近于0，导致梯度消失或爆炸**。这种情况下，神经网络可能会面临训练困难、收敛缓慢等问题。

常见的饱和性质的激活函数有Sigmoid函数和双曲正切（Tanh）函数。它们在输入接近极端值时，导数接近于0。对于Sigmoid函数而言，在输入非常大或非常小时，输出值会趋向于1或-1，并且导数几乎为0；对于Tanh函数而言，在输入非常大或非常小时，输出值也会趋向于1或-1，并且导数同样几乎为0。

相比之下，不饱和性质的激活函数没有上述问题并具有更好的表达能力。