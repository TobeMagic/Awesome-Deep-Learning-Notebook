# 分类损失函数

## 分类交叉熵

分类交叉熵（categorical cross-entropy）是一种用于多分类问题的损失函数。它的计算基于模型的预测概率分布和真实标签的one-hot向量，其中每个元素表示样本属于对应类别的概率，只有一个元素为1，其余元素为0。假设有n个样本，每个样本有k个可能的类别，则模型的预测结果可以表示为一个形状为(n, k)的概率矩阵，真实标签可以表示为一个形状为(n, k)的one-hot矩阵。

分类交叉熵的计算公式如下：

$ J(\theta) = -\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^k y_{ij} \log(p_{ij}) $

其中，$y_{ij}$是样本i属于类别j的one-hot标签，$p_{ij}$是模型预测样本i属于类别j的概率。公式中的第一个求和符号遍历所有样本，第二个求和符号遍历所有类别。

分类交叉熵的含义是用来评估模型预测结果与真实标签之间的差异。当模型预测结果和真实标签一致时，交叉熵为0；当它们之间的差距越大时，交叉熵的值越大。因此，分类交叉熵可以作为模型的损失函数，用来优化模型的参数。

在实际训练中，分类交叉熵通常与反向传播算法结合使用，以计算每个参数的梯度并更新参数。可以使用基于梯度下降的优化算法，如随机梯度下降（SGD）或Adam来最小化分类交叉熵损失，以提高模型的分类精度。

## 稀疏分类交叉熵

稀疏分类交叉熵和普通的分类交叉熵在计算损失函数的方式上有所不同。

在普通的分类交叉熵中，目标值（即真实标签）采用了one-hot编码的方式，即只有真实类别对应的那个元素为1，其余元素都为0。在计算损失函数时，需要将神经网络输出的概率分布和目标值进行对比，计算交叉熵损失。

而在稀疏分类交叉熵中，目标值采用的是整数形式的标签，即每个样本的真实标签对应一个整数值。这种方式避免了使用one-hot编码所产生的大量零元素，节省了存储空间。在计算损失函数时，需要将神经网络输出的概率分布和目标值进行对比，计算交叉熵损失。

因此，稀疏分类交叉熵和普通的分类交叉熵的主要区别在于目标值的表示方式不同，稀疏分类交叉熵适合处理类别数目较多的情况，能够更有效地处理稀疏数据。

> 假设有一个图像分类问题，共有10个类别。使用稠密编码（dense encoding），每个样本的标签是一个长度为10的向量，每个元素代表一个类别，值为1表示属于该类别，值为0表示不属于。比如样本1属于第5个类别，则其标签为[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]。
>
> 使用稀疏编码（sparse encoding），每个样本的标签是一个标量，表示该样本所属的类别。比如样本1属于第5个类别，则其标签为4（从0开始计数）。在训练模型时，使用稀疏编码可以节省存储空间和计算资源。
>
> 对于稠密编码，其分类交叉熵的计算方式为：
>
> $-\sum_{i=1}^{10} y_i \log(\hat{y_i})$
>
> 其中，$y_i$是样本的真实标签中第$i$个元素的值，$\hat{y_i}$是模型预测的该元素的概率值。
>
> 对于稀疏编码，其分类交叉熵的计算方式为：
>
> $-\log(\hat{y_j})$
>
> 其中，$j$是样本的真实标签。由于真实标签只有一个元素为1，因此只需要计算该元素对应的预测概率值的对数即可。

## 二元熵

Binary Cross-entropy（二元交叉熵）是一种衡量两个概率分布相似性的方法，常用于二分类问题中。在机器学习中，它通常被用来衡量预测值与实际值之间的差异。

举个例子，假设我们有一个二分类问题，数据标签为0或1。我们的模型输出的预测值为y_pred，实际值为y_true。Binary Cross-entropy可以通过以下公式计算：

$H(p,q)=-\frac{1}{N}\sum_{i=1}^{N}[y_i\log(p_i)+(1-y_i)\log(1-p_i)]$

其中，p表示模型输出的概率分布，q表示实际的标签分布。N表示样本数量，yi表示第i个样本的标签。

Binary Cross-entropy可以解释为模型对于一个样本预测为正类的不确定性，即模型预测正类的概率是多少。当预测越接近实际标签时，交叉熵越小，当预测与实际标签不一致时，交叉熵越大。

这个公式也可以扩展到多分类问题中，称为Categorical Cross-entropy。它的计算方法类似，不同之处在于标签使用独热编码。