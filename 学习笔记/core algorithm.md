### 卷积神经网络

确定卷积层的最佳数量以及它们对其他参数的影响是一个挑战性的任务，通常需要进行实验和调整来找到最佳的模型架构。

一般而言，卷积层数量的选择可以基于以下因素进行评估：

1. **数据集大小**和复杂程度：更大更复杂的数据集可能需要更深的卷积神经网络，以便提取更丰富的特征。较小的数据集则可能需要较浅的卷积神经网络，以避免过拟合。

2. 训练时长和计算资源：更深的卷积神经网络需要更长的训练时间和更多的计算资源。在限制时间和计算资源的情况下，可能需要权衡深度和精度。

3. 预训练模型的可用性：使用预训练模型可以减少训练时间并提高模型的精度。如果可用的预训练模型包含了与问题相关的卷积层，则可以考虑从这些层开始，然后通过微调来逐步优化模型。

除了卷积层的数量外，其他参数也会影响模型的性能。例如，卷积层的大小、步幅、填充等参数会影响特征图的大小和数量。池化层的类型、大小和步幅也会影响特征图的大小和数量，从而影响后续层的表现。因此，在设计卷积神经网络时，需要综合考虑这些参数，并进行实验和调整以找到最佳的模型结构。

> Q: CNN 1D与1至5个卷积层模型测试性能获得的准确性分别为88.36%、89.48%、88.86%、87.96和86.89%。五个1D CNN层是最大的界限，因为这个层上的函数图的最小尺寸已经超过了。
>
> A: 这个问题涉及到卷积神经网络中的**感受野（receptive field）概念**。
>
> 在卷积神经网络中，每一层的卷积核实际上是对上一层特征图的局部区域进行处理，而**该局部区域的大小就是该层的感受野大小**。这意味着，随着层数的增加，感受野也会逐渐扩大。
>
> 在1D CNN中，每个卷积核只能查看其左右固定数目的元素，这个固定数目就是感受野。因此，通过堆叠多个1D CNN层，可以使得后面的层拥有更大的感受野，从而提取更全局的特征。
>
> 但是，当1D CNN层数过多时，每一层的输出的长度也会逐步缩小。这是因为，在1D CNN中，卷积操作将输入向量的每个元素映射到输出向量的一个元素，因此每次卷积操作都会减少向量长度。随着层数的增加，输出向量的长度也会逐渐缩小，最终可能会导致信息丢失，从而影响模型性能。
>
> 因此，作者在该问题中使用了1至5个1D CNN层进行测试，并发现5层是极限。作者指出，当使用5个1D CNN层时，**最后一层的输出长度已经非常短，无法再添加更多的卷积层**。因此，**作者不能通过增加层数来进一步提高模型性能，而必须尝试其他方法，如调整卷积核大小、池化方式等**，以达到更好的性能。

### Transformer

注意力（Attention）机制由Bengio团队与2014年提出并在近年广泛的应用在深度学习中的各个领域，例如在计算机视觉方向用于捕捉图像上的感受野，或者NLP中用于定位关键token或者特征。谷歌团队近期提出的用于生成词向量的BERT[3]算法在NLP的11项任务中取得了效果的大幅提升，堪称2018年深度学习领域最振奋人心的消息。而BERT算法的最重要的部分便是本文中提出的Transformer的概念。

1. 背景和动机：作者采用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：

   1. 时间片 t 的计算依赖 t−1 时刻的计算结果，这样限制了模型的并行能力；

   2. 传统的序列模型（如循环神经网络）存在着长期依赖问题，难以捕捉长距离的依赖关系。顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。

      > 长期依赖关系指的是在序列数据中存在较大时间间隔的依赖性。具体而言，在处理序列数据时，过去的信息对于当前和未来的预测具有重要影响。然而，传统的序列模型（如循环神经网络）在计算过程中存在**梯度消失或梯度爆炸问题，导致难以捕捉到长距离的依赖关系**。
      >
      > 当序列长度较长时，例如几十个或几百个时间步长，信息在每个时间步长传递过程中会经历多次连续的转换。在这个过程中，每个时间步长的输入都会通过一系列的乘法操作进行计算，其中包含一个权重矩阵，即循环神经网络中的参数。梯度在反向传播时通过这些乘法操作进行传递，每次乘法都可能使梯度变小或变大。
      >
      > 梯度消失问题意味着在反向传播过程中，梯度逐渐变小并最终趋近于零，导致远距离时间步长的信息无法有效更新。因此，模型可能无法捕捉到与该信息相关的长期依赖关系。相反，梯度爆炸问题指的是梯度在反向传播过程中变得非常大，导致数值不稳定和训练困难。
      >
      > 为了缓解长期依赖问题，LSTM（长短期记忆网络）引入了门控机制，通过选择性地遗忘和更新信息来控制梯度的流动。LSTM使用了一个称为"遗忘门"的结构，它可以**决定将多少过去的信息保留下来，并通过添加"输入门"决定引入多少新信息**。这些门控机制帮助LSTM模型更好地捕捉长期依赖关系，但**对于特别长期的依赖现象，LSTM仍然可能无法完全解决问题**。
      >
      > 近年来，一些新的序列模型如Transformer和GPT（生成式预训练模型）已经取得了重大突破。它们采用了自注意力机制，通过直接建模序列中所有位置之间的关系，能够更有效地捕捉长距离的依赖关系，从而提升了序列模型在处理长期依赖问题上的能力。

2. 自注意力机制（Self-Attention）：是 Transformer 模型的核心组件之一。自注意力允许模型根据输入序列中不同位置的相关性权重来计算每个位置的表示。通过计算查询、键和值之间的相似性得分，并将这些得分应用于值来获取加权和，从而生成每个位置的输出表示。

3. 编码器-解码器结构：Transformer 模型通常包含一个编码器和一个解码器。编码器负责将输入序列转换为一系列高级特征表示，而解码器则根据这些特征预测输出序列。编码器和解码器都由多层的自注意力层和前馈神经网络层组成。

4. 位置编码：由于 Transformer 模型没有显式的顺序信息，为了保留输入序列的位置信息，需要引入位置编码。位置编码是一种向输入嵌入中添加的特殊向量，用于表示单词或标记在序列中的位置。

5. 多头注意力：Transformer 模型允许同时使用多个自注意力机制，每个注意力机制被称为一个头（head）。通过并行计算多个头，模型可以学习不同粒度和关注不同方面的特征表示。

6. 前馈神经网络层：除了自注意力层之外，Transformer 模型还包括前馈神经网络层。该层由两个全连接层组成，并通过应用激活函数（如 ReLU）来处理输入特征。

7. 残差连接和层归一化：Transformer 模型使用残差连接（residual connections）来使梯度更容易传播，以及层归一化（layer normalization）来加速训练过程和提高模型性能。

8. 训练和推理：Transformer 模型通常使用基于自回归的训练方法进行训练，其中解码器的每个位置只能看到先前位置的输出。在推理阶段，可以利用已生成的部分序列作为输入来预测下一个标记，直到生成完整序列。

总之，Transformer 模型通过引入自注意力机制和并行计算的方式解决了传统序列模型中的长期依赖问题。它在自然语言处理任务中表现出色，并成为了许多重要NLP任务的基础，如机器翻译、文本生成和语言理解等。





### BP神经网络 & 感知器 (Percepton)

1、BP神经网络，指的是用了**“BP算法”进行训练的“多层感知器模型”（误差反向传播，back propagation)。**并为了该算法正常工作，对MLP的架构进行了修改，即将阶跃函数替换成其他激活函数，如`tanh`，`Relu`。
2、感知器是最简单的ANN架构之一，在一开始的生物神经元模型，即简单逻辑运算的神经元（根据神经元是否被激活进行交集并集，或 非 运算），后来感知器在1977年由Frank Roseblatt所发明，称之为阈值逻辑单元（TLU，threshold logistic unit)，也被称为线性阈值单元（LTU,linear threshold unit)，其是一个使用阶跃函数的神经元（这个阶跃函数即通过与0的比较，确定是 0， 1 或者 -1 ）来计算（我猜想之所以一开始用阶跃函数是因为前面的逻辑运算思维，因为逻辑运算就只有0，1两种情况。因为本身的原理，**单个TLU 单元可被用于二分类任务**，多个就可以实现多分类任务，只需要设置输出TLU个数为类别个数即可，以输出n个二进制结果，一般来说还会添加一个偏置特征1来增加模型灵活性

> 在感知器中引入一个偏置特征1的目的是为了增加模型的灵活性和表达能力。这个偏置特征对应于一个固定且始终为1的输入，**其对应的权重称为偏置项（bias）。通过调整偏置项的权重，我们可以控制 TLU 的决策边界在特征空间中平移或倾斜**。(正常来说的话，这个偏置项都是在每个神经元当中所存在，而不是作为单独一个输入存在，能更灵活)
>
> > 在感知器中，将偏置特征固定为1的选择是为了方便计算和表示。
> >
> > 当我们引入一个偏置特征时，可以将其视为与其他输入特征一样的维度，并赋予它一个固定的值1。这样做有以下几个好处：
> >
> > 1. 方便计算：将偏置项乘以1相当于直接使用权重来表示该偏置项。在进行加权求和并应用阈值函数时，不需要额外操作或考虑。
> > 2. 参数统一性：通过将偏置项作为一个独立的权重进行处理，使得所有输入特征（包括原始输入和偏置）具有相同的形式和统一性。
> > 3. 简洁明了：固定为1的偏置特征能够简化模型参数表示，并使其更易理解和解释。
> >
> > 请注意，在实际应用中，对于某些任务可能需要调整默认值1以适应数据分布或优化模型性能。但基本原则仍然是保持一个常数值作为额外输入特征，并且通常会根据具体情况对其进行学习或调整。
>
> 具体来说，引入偏置特征1有以下几个原因：
>
> 1. 平移决策边界：通过调整偏置项的权重，可以使得决策边界沿着不同方向平移。如果没有偏置项，则决策边界将必须过原点(0, 0)。
> 2. 控制输出截距：当所有其他输入都为零时，只有存在偏置项才能使感知器产生非零输出。
> 3. 增强模型表达能力：引入一个额外维度后，在某些情况下会更容易找到合适分割样本空间线性超平面位置。
>
> 总之，在感知器中引入偏置特征1可以使模型更加灵活，能够适应不同的决策边界位置，并增加了模型对输入数据的表达能力。





从以下我们就可以看到线性可分的感知机训练过程和线性不可分的感知机训练过程，在线性不可分的情况下，泛化能力较差。

![img](classical algorithm.assets/20190112221655864.gif)

![img](classical algorithm.assets/20190112221824187.gif)



