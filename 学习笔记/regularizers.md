## 岭回归正则化 & `lesso`正则化

>  常见的正则化有岭回归正则化，`lesso`正则化，但本质就是L1正则化或L2正则化

### 原理

正则化的alpha值一般需要通过实验调参来确定，通常使用交叉验证等技术来选择最优的超参数。不同的问题可能需要不同的alpha值。

L1正则化的公式为：$\lambda \sum_{i=1}^{n}|w_i|$

L2正则化的公式为：$\lambda \sum_{i=1}^{n}w_i^2$

其中，$w_i$表示模型参数，$\lambda$表示正则化参数。L1正则化会使模型的参数更加稀疏，适合于特征选择；L2正则化则可以有效防止过拟合，适合于参数的约束。

> 特征选择和参数约束是两种不同的方法，用于减少模型的复杂度和提高泛化能力。
>
> 特征选择适用于**特征维度过高的情况，即特征数量很多**，但其中只有部分特征与目标变量有关系。在这种情况下，去除无关的特征可以降低噪声和提高模型的泛化能力。一些常见的特征选择方法包括**卡方检验、相关系数、互信息等**。
>
> 参数约束适用于模型过于复杂，需要对模型的参数进行约束以避免过拟合的情况。常见的参数约束方法包括L1和L2正则化，Dropout等。L1正则化和L2正则化通过给模型的损失函数添加一个参数惩罚项来约束模型的参数，L1正则化会使一部分参数变成0，从而达到特征选择的效果，L2正则化则会让参数值变小，从而降低模型的复杂度。
>
> 具体使用哪种方法取决于数据的特征、模型的结构以及需要解决的问题。一般来说，特征选择适用于特征维度过高的情况，而参数约束适用于模型过于复杂的情况。	

### 选择

选择使用哪种正则化（L1正则化或L2正则化）通常取决于具体情况。一般来说，L2正则化常常能够更好地控制权重的大小，从而防止过拟合。而L1正则化则更倾向于将一些特征的权重归零，这有助于特征选择和模型的解释性。

在选择正则化参数alpha时，通常需要在训练集上进 行交叉验证。通过在不同的alpha值上训练模型，并在验证集上评估模型的性能，可以选择最优的alpha值。通常情况下，alpha值越大，正则化的强度越高，权重的大小会更加受限制。但是，如果alpha太大，会导致权重过于受限制而影响模型的拟合能力。