《 A review on the attention mechanism of deep learning 》 经典论文

人类的注意力机制根据其生成方式可以分为两类[3]。第一类是自下而上的无意识注意力，称为基于显着性的注意力，它是由外部刺激驱动的。人们在谈话时更有可能听到大声的声音。它类似于深度学习中的最大池化和门控机制[4,5]，将更合适的值（即更大的值）传递到下一步。第二类是自上而下的有意识的注意力，称为集中注意力。集中注意力是指具有预定目的并依赖于特定任务的注意力。它使人类能够有意识地、主动地将注意力集中在某个物体上。深度学习中的注意力机制大多是根据具体任务来设计的，因此大部分都是集中注意力的。本文介绍的注意力机制，除特殊陈述外，通常指的是集中注意力。	如上所述，注意力机制可以作为一种资源分配方案，是解决信息过载问题的主要手段。在计算能力有限的情况下，可以用有限的计算资源处理更重要的信息

## Softmax

基于softmax实现的注意力机制网络是一种用于加权聚焦输入序列中不同位置信息的方法。在这里，我们以BLSTM的输出为例来详细解释。

注意力机制的目标是让模型能够在处理输入序列的同时，动态地选择性地关注 序列中的不同位置，并对不同位置的信息进行加权处理，以便更好地捕捉有关任务的重要特征。

对于BLSTM的输出，我们假设其为一个时间步长为 \(T\) 的序列，记为 $(\mathbf{H} = \mathbf{h}_1, \mathbf{h}_2, ..., \mathbf{h}_T$)，其中 $\mathbf{h}_t$ 为BLSTM在时间步 \(t\) 的输出（即前向和反向LSTM输出的拼接）。

注意力机制的过程如下：(注意这里是对每个时间步的序列的不同位置进行注意力)

1. 注意力权重计算：

首先，我们引入一个可学习的注意力权重参数矩阵$(\mathbf{W_a} \in \mathbb{R}^{d_a \times 2d_h})$，其中 $d_a$ 是注意力权重的维度，$2d_h$ 是BLSTM输出的维度，即前向和反向LSTM的隐藏状态的维度之和。

然后，对于每个时间步 $t$，计算对应的注意力权重 $e_t$，通常采用以下的计算方式：

$e_t = \text{softmax}(\mathbf{W_a} \mathbf{h}_t)$

注意力权重 $e_t$ 是一个长度为 $T$ 的向量，它表示模型在时间步 \(t\) 对整个序列各个位置的关注程度。

2. 加权求和：

接下来，我们将注意力权重 $e_t$与BLSTM的输出序列 \(\mathbf{H}\) 进行加权求和，得到注意力加权的表示向量 $(\mathbf{c})$：

$\mathbf{c} = \sum_{t=1}^{T} e_t \cdot \mathbf{h}_t$

注意力加权的表示向量 \mathbf{c}$ 用于捕捉输入序列中关键信息。

3. 注意力输出：

最后，我们可以将注意力加权的表示向量 $\mathbf{c}$ 作为注意力机制网络的输出，供后续任务进行进一步处理或预测。

通过注意力机制，模型能够根据输入序列的内容自适应地调整权重，更好地捕捉与任务相关的信息，提升了模型在处理长序列数据时的性能。

需要注意的是，以上是基于softmax实现的注意力机制的一种简单示例，实际应用中还有其他变种和改进。注意力机制在自然语言处理、图像处理等领域都得到了广泛的应用和研究。

## Self-attention

见 transformer部分 [core algorithm.md](core algorithm.md) 