《 A review on the attention mechanism of deep learning 》 经典论文

人类的注意力机制根据其生成方式可以分为两类[3]。第一类是自下而上的无意识注意力，称为基于显着性的注意力，它是由外部刺激驱动的。人们在谈话时更有可能听到大声的声音。它类似于深度学习中的最大池化和门控机制[4,5]，将更合适的值（即更大的值）传递到下一步。第二类是自上而下的有意识的注意力，称为集中注意力。集中注意力是指具有预定目的并依赖于特定任务的注意力。它使人类能够有意识地、主动地将注意力集中在某个物体上。深度学习中的注意力机制大多是根据具体任务来设计的，因此大部分都是集中注意力的。本文介绍的注意力机制，除特殊陈述外，通常指的是集中注意力。	如上所述，注意力机制可以作为一种资源分配方案，是解决信息过载问题的主要手段。在计算能力有限的情况下，可以用有限的计算资源处理更重要的信息

“注意力”在平时的生活中相信大家都深有体会，当你认真读某本书的时候，会感觉眼睛中只有书中正在读的文字，竖起耳朵去听一个很微弱的声音的时候，这个声音也仿佛放大了，能听的更清楚。深度学习中Attention机制非常类似生物的注意力。人们视觉在感知东西的时候一般不会是一个场景从到头看到尾每次全部都看，而往往是根据需求观察注意特定的一部分。而且当人们发现一个场景经常在某部分出现自己想观察的东西时，人们会进行学习在将来再出现类似场景时把注意力放到该部分上。

<img src="https://github.com/EvilPsyCHo/Attention-PyTorch/raw/master/asset/attention/visual1.jpg" alt="img" style="zoom:50%;" />

## 基于Softmax的Attention

基于softmax实现的注意力机制网络是一种用于加权聚焦输入序列中不同位置信息的方法。在这里，我们以BLSTM的输出为例来详细解释。

注意力机制的目标是让模型能够在处理输入序列的同时，动态地选择性地关注 序列中的不同位置，并对不同位置的信息进行加权处理，以便更好地捕捉有关任务的重要特征。

对于BLSTM的输出，我们假设其为一个时间步长为 \(T\) 的序列，记为 $(\mathbf{H} = \mathbf{h}_1, \mathbf{h}_2, ..., \mathbf{h}_T$)，其中 $\mathbf{h}_t$ 为BLSTM在时间步 \(t\) 的输出（即前向和反向LSTM输出的拼接）。

注意力机制的过程如下：(注意这里是对每个时间步的序列的不同位置进行注意力)

1. 注意力权重计算：

首先，我们引入一个可学习的注意力权重参数矩阵$(\mathbf{W_a} \in \mathbb{R}^{d_a \times 2d_h})$，其中 $d_a$ 是注意力权重的维度，$2d_h$ 是BLSTM输出的维度，即前向和反向LSTM的隐藏状态的维度之和。

然后，对于每个时间步 $t$，计算对应的注意力权重 $e_t$，通常采用以下的计算方式：

$e_t = \text{softmax}(\mathbf{W_a} \mathbf{h}_t)$

注意力权重 $e_t$ 是一个长度为 $T$ 的向量，它表示模型在时间步 \(t\) 对整个序列各个位置的关注程度。

2. 加权求和：

接下来，我们将注意力权重 $e_t$与BLSTM的输出序列 \(\mathbf{H}\) 进行加权求和，得到注意力加权的表示向量 $(\mathbf{c})$：

$\mathbf{c} = \sum_{t=1}^{T} e_t \cdot \mathbf{h}_t$

注意力加权的表示向量 $\mathbf{c}$ 用于捕捉输入序列中关键信息。

3. 注意力输出：

最后，我们可以将注意力加权的表示向量 $\mathbf{c}$ 作为注意力机制网络的输出，供后续任务进行进一步处理或预测。

通过注意力机制，模型能够根据输入序列的内容自适应地调整权重，更好地捕捉与任务相关的信息，提升了模型在处理长序列数据时的性能。

## Multi-head & Self-attention

见 transformer部分 [core algorithm.md](core algorithm.md) 

## Squeeze-and-Excitation（SE） 

Squeeze-and-Excitation（SE）注意力是一种用于增强卷积神经网络（CNN）中特征表示的子网络。该方法在2018年由胡杰、申力和孙刚在《Squeeze-and-Excitation Networks》一文中提出。

SE注意力的目的是在CNN中有选择地强调信息丰富的特征，同时抑制不相关的特征。它旨在通过自适应地重新校准特征图来提高模型的区分能力和泛化能力。

下面是对Squeeze-and-Excitation注意力子网络的组成部分和工作原理的详细解释：

1. **Squeeze操作**：Squeeze操作对输入特征图执行全局空间池化。它将特征图的空间维度降低为一个通道，以捕获全局信息。这个池化操作可以是平均池化或最大池化，具体实现取决于具体情况。
2. **Excitation操作**：Excitation操作旨在建模通道间的依赖关系，并捕获不同通道之间的相互依赖性。它由两个全连接（FC）层和非线性激活函数组成。第一个FC层通过一个预先设置的减少比例来减少通道的数量。第二个FC层将通道数量增加回原始大小。这些FC层学习捕获通道之间的关系，并计算每个通道的重要性分数。
3. **缩放和门控**：最后一步是根据Excitation操作得到的重要性分数对原始特征图进行缩放。这通过将特征图与通道权重进行逐元素相乘来实现。这个操作允许网络有选择地放大或抑制特征图的不同通道，以强调信息丰富的特征并抑制不相关的特征。

Squeeze-and-Excitation注意力子网络可以无缝地集成到不同的CNN架构中。它可以在卷积层之前或之后作为一个构建模块添加。通过自适应地重新校准特征图，SE注意力机制帮助网络专注于相关的特征，在图像分类、目标检测和语义分割等各种计算机视觉任务中提高模型性能。

SE注意力机制在提高模型准确性方面表现出色，且计算成本仅略微增加。其简单性和灵活性使其成为增强CNN特征表示的流行选择。