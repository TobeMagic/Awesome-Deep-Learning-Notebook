## RMSProp

`RMSProp`算法主要用于解决梯度下降算法中学习率的问题，它的核心思想是自适应地调整每个参数的学习率。具体而言，`RMSProp`算法计算**每个参数的平方梯度的指数移动平均值**，然后将**学习率除以该指数移动平均值的平方根**，从而使**每个参数的学习率根据其梯度的大小进行自适应调整**。(参数移动越大，学习率越小)

`RMSProp`算法可以避免学习率在训练期间过于波动，以及过度更新导致的问题，使得梯度下降算法更加稳定。

RMSProp的核心思想是对梯度进行指数加权平均，并使用平均值来调整学习率。通过计算梯度平方的指数移动平均，RMSProp可以自适应地调整每个参数的学习率。具体来说，RMSProp维护一个额外的变量，即平方梯度的指数移动平均值。在每次迭代中，它计算当前梯度的平方，并将其乘以一个衰减因子，然后将其加到平均值中。这样可以使得较大梯度的影响减小，较小梯度的影响增加，从而更好地适应不同参数的学习率。

下面是RMSProp优化器的算法步骤：

1. 初始化参数：初始学习率lr，衰减因子decay，小常数epsilon（用于数值稳定性）。
2. 初始化累积平方梯度变量`cache`为0。
3. 在每次迭代中，计算当前迭代的梯度g。
4. 更新累积平方梯度变量：`cache = decay * cache + (1 - decay) * g^2`。
5. 计算参数的更新量：`delta = -lr * g / sqrt(cache + epsilon)`。
6. 更新参数：`param = param + delta`。
7. 重复步骤3-6，直到达到停止条件（例如达到最大迭代次数或达到预定的性能指标）。

RMSProp的优点是能够自适应地调整学习率，适用于不同参数的优化。它对于处理具有稀疏梯度的问题也比较有效。此外，RMSProp还可以通过减小学习率衰减因子来提高模型的收敛速度。

然而，RMSProp也有一些缺点。例如，它可能会受到初始学习率的选择影响，过高或过低的学习率都可能导致模型性能下降。此外，RMSProp在处理非平稳问题时可能遇到困难，因为它没有显式地保留历史梯度信息。

## Momentum GD

动量梯度下降（Momentum Gradient Descent）是一种优化算法，用于训练神经网络和深度学习模型。它在标准梯度下降算法的基础上引入了动量的概念，旨在加速模型的收敛速度并提高优化的稳定性。

在标准梯度下降中，参数的更新仅取决于当前的梯度。然而，在动量梯度下降中，除了当前梯度，还引入了一个称为动量（momentum）的变量，用于表示之前梯度的累积信息。动量可以理解为参数更新的速度和方向。

下面是动量梯度下降的算法步骤：

1. 初始化参数：初始学习率lr，动量因子momentum。
2. 初始化参数更新量`delta`为0。
3. 在每次迭代中，计算当前迭代的梯度g。
4. 更新参数更新量：`delta = momentum * delta - lr * g`。
5. 更新参数：`param = param + delta`。
6. 重复步骤3-5，直到达到停止条件（例如达到最大迭代次数或达到预定的性能指标）。

动量梯度下降的关键是在参数更新时结合了当前梯度和之前梯度的信息。动量因子决定了之前梯度在更新中的影响程度。当动量因子较大时，之前的梯度信息对更新的影响更大，可以加速收敛速度；当动量因子较小时，更注重当前梯度，可以提高优化的稳定性。

使用动量梯度下降的优点是可以加速模型的收敛速度，并且对于处理具有高曲率或小尺度梯度的问题效果更好。它可以帮助模型在参数空间中跳出局部最小值，并朝着全局最小值方向进行更新。

需要注意的是，动量梯度下降也有一些考虑因素。过大的学习率和动量因子可能导致模型震荡或发散。因此，在使用动量梯度下降时，需要对学习率和动量因子进行适当的调整和选择。

## 	Adam

### 介绍

Adam（Adaptive Moment Estimation）算法是一种优化算法，用于梯度下降过程中的参数更新，特别是在深度学习模型中广泛使用。

Adam算法是基于梯度下降算法的变体，结合了**动量梯度下降和RMSProp算法**的优点，旨在通过自适应调整学习率和动量，加速训练并获得更好的结果。

具体而言，Adam算法通过计算**每个参数的指数移动平均值**和**平方梯度的指数移动平均值**来更新参数。它包含以下步骤：

1. 初始化参数：设置学习率、动量参数和平方梯度的指数移动平均参数。
2. 计算梯度：计算当前参数的梯度。
3. 计算动量：计算动量向量，考虑当前梯度和上一次动量向量。
4. 计算平方梯度的指数移动平均值：计算当前参数的平方梯度，使用指数移动平均来计算平方梯度的指数移动平均值。
5. 更新参数：使用动量和平方梯度的指数移动平均值来更新参数。
6. 重复步骤2-5，直到达到最大迭代次数或收敛。

Adam算法有许多超参数可以调整，如学习率、动量参数和平方梯度的指数移动平均参数等。通常情况下，可以使用默认值或根据经验进行调整。

Adam算法在深度学习中表现出色，具有较快的收敛速度和较好的泛化性能。然而，它并不适用于所有类型的问题，可能需要根据具体问题进行优化算法选择。

###  缺点

Adam算法在深度学习中表现优异，但并不适用于所有类型的问题。以下是一些可能不适合使用Adam算法的情况：

1. 数据集较小：当训练数据集很小或具有噪声时，Adam算法可能会在噪声中迷失方向，导致模型性能下降。
2. 学习率较高：当学习率设置过高时，Adam算法可能会导致参数更新过于频繁，使得模型在训练期间发生震荡和不稳定。
3. 非凸优化问题：在非凸优化问题中，Adam算法可能会在局部最小值处陷入困境，而无法找到全局最小值。

对于这些问题，可以考虑使用其他优化算法来解决，例如：

1. 数据集较小：可以使用随机梯度下降（Stochastic Gradient Descent，SGD）算法，通过在每个批次上进行梯度下降来避免过拟合。
2. 学习率较高：可以使用`Adagrad`算法，它可以自适应地调整每个参数的学习率，避免过度更新。
3. 非凸优化问题：可以使用L-BFGS算法等二阶优化算法，以快速找到全局最小值。

总之，选择优化算法应该考虑具体问题和数据集的特征，并根据实验结果进行调整和优化。

## AdaMax