# 回归损失函数

在面对目标是不同的分布形态时，损失函数的选择有着不同的效果。比如在统计学中，单峰概率分布或单峰分布是具有单峰的概率分布。

<img src="https://i.stack.imgur.com/E89Ge.jpg" alt="number and locations of modes" style="zoom:50%;" />

当面对一个连续的回归目标时，很容易想到将L1或L2损失直接应用到原始目标上，看它效果如何，如果回归目标是单峰的（unimodal），L2范数应该工作得很好. 然而，如果目标是超过单峰的，可能会发现 L2 损失会产生不良结果

如下图未能使用 L2 损失对双峰数据分布进行回归：

<img src="https://miro.medium.com/v2/resize:fit:785/0*U8ZklXhNlX_nkjhI.png" alt="img" style="zoom:50%;" />

在大范围且超出类高斯目标分布的情况下

需要注意的是L1 损失也好不到哪儿去。 L2 损失假设高斯先验，L1 损失假设拉普拉斯先验，这也是一种单峰分布。直观上，平滑 L1 损失或 Huber 损失（L1 和 L2 损失的组合）也假设单峰基础分布。

**首先将回归目标的分布可视化，并考虑比L2更能反映和容纳目标数据分布的其他损失函数，才是更加合理的做法**. 例如，如果目标分布是双峰的，一种直观的方法是查找目标属于哪个模式(或bin)，然后对距模态中心的偏移进行回归.

> 这正是所谓的多bin损失(或混合分类/回归损失，离散/连续损失)所做的. 在CVPR 2017年的论文《3D Bounding Box Estimation Using Deep Learning and Geometry》中，首次提出了这种损失用于角度回归. 最初的背景是在单眼3D车辆检测中返回一个范围在[-π, π)的连续方向角. 从那时起，它被广泛应用于三维物体检测中的汽车方向回归，包括仅使用单眼图像(如Multi-level fusion、MonoPSR和FQNet)和使用点云(如Frustum PointNet和AVOD).

参考文章：
原文：https://towardsdatascience.com/anchors-and-multi-bin-loss-for-multi-modal-target-regression-647ea1974617
翻译：https://blog.csdn.net/Cratial/article/details/110792169

## 均方误差损失（MSE）& L2

## L1（MAE）

# 分类损失函数

## 均方误差损失（MSE）

均方误差损失（Mean Squared Error, MSE）。**均方误差损失又称为二次损失、L2损失，常用于回归预测任务中**，它也可以用于衡量模型在每个类别上预测概率与实际标签之间的差异。具体来说，对于每个样本，该损失函数计算了预测概率与相应独热编码值之间的平方差，并将所有类别的平方差求和。

> MSE 和 信息熵在区别上就是对预测类别使用平方和对数的区别，和决策树中的增益中**基尼指数和信息熵增益**之前的区别是一样的。

让我们以一个简单的例子来说明这个损失函数：

假设有一个三分类问题，其中每个样本都有一个真实标签（例如：[1, 0, 0]表示第一类）和一个模型预测出来的概率分布（例如：[0.8, 0.1, 0.1]）。根据这个损失函数计算过程如下：

- 第一个类别：(0.8 - 1)^2 = 0.04
- 第二个类别：(0.1 - 0)^2 = 0.01
- 第三个类别：(0.1 - 0)^2 = 010

最后将所有类别得到的平方差进行累加：
总损失 = (0.04 + 0.01 + 0.10) = 0.15

需要注意的是，**MSE损失函数对异常值比较敏感，因为它将平方差进行求和**。在某些情况下，可能会使用其他类型的损失函数（如交叉熵）来解决特定问题或改善模型训练效果。

## 交叉熵

> 熵是信息理论中的一个概念，它用于衡量随机变量的不确定性或信息量。在信息论中，熵常常被表示为对数函数的形式。交叉熵是一种用于**衡量两个概率分布之间差异的指标**。在机器学习中，交叉熵常用于损失函数的计算。
>
> 假设我们有两个概率分布 *p* 和 *q*，其中 *p* 表示真实分布，*q* 是模型预测的分布。我们希望通过某种方式衡量 *p* 和 *q* 之间的差异。
>
> 交叉熵的定义如下：
>
> $H(p, q) = - \sum_{x} p(x) \log q(x)	$  
>
> 其中 x 是概率分布所涉及的事件或类别，*p*(*x*) 表示真实分布中 *x* 的概率，*q*(*x*) 表示模型预测中 *x* 的概率。交叉熵的值越小，表示 *p* 和 *q* 之间的差异越小，模型的预测也就越准确。
>
> 其中log可以是以任何基数为底的对数函数（通常使用自然对数——以e为底），这样计算出来的单位是“比特”或“纳特”。当log函数以2为底时（即二进制对数），则计算出来的单位是“比特”，因此也称之为二进制熵。
>
> 总结起来，虽然在计算**熵时使用了log函数作为一种标准方式**，并且**通常使用自然对数进行计算**（即以e为底），但并不意味着熵只能通过log函数进行定义和计算。

### 二元交叉熵

Binary Cross-entropy（二元交叉熵）是一种衡量两个概率分布相似性的方法，常用于二分类问题中。在机器学习中，它通常被用来衡量预测值与实际值之间的差异。

举个例子，假设我们有一个二分类问题，数据标签为0或1。我们的模型输出的预测值为y_pred，实际值为y_true。Binary Cross-entropy可以通过以下公式计算：

$H(p,q)=-\frac{1}{N}\sum_{i=1}^{N}[y_i\log(p_i)+(1-y_i)\log(1-p_i)]$

其中，p表示模型输出的概率分布，q表示实际的标签分布。N表示样本数量，yi表示第i个样本的标签。

Binary Cross-entropy可以解释为模型对于一个样本预测为正类的不确定性，即模型预测正类的概率是多少。当预测越接近实际标签时，交叉熵越小，当预测与实际标签不一致时，交叉熵越大。

**这个公式也可以扩展到多分类问题中，称为Categorical Cross-entropy**。它的计算方法类似，**不同之处在于标签使用独热编码。**

### 多分类交叉熵

分类交叉熵（categorical cross-entropy）是一种用于多分类问题的损失函数。它的计算基于模型的预测概率分布和真实标签的one-hot向量，其中每个元素表示样本属于对应类别的概率，只有一个元素为1，其余元素为0。假设有n个样本，每个样本有k个可能的类别，则模型的预测结果可以表示为一个形状为(n, k)的概率矩阵，真实标签可以表示为一个形状为(n, k)的one-hot矩阵。

分类交叉熵的计算公式如下：

$\text{Cross-Entropy} = -\sum_{i=1}^{C} y_i \log(p_i) $

$ J(\theta) = -\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^k y_{ij} \log(p_{ij}) $

其中，$y_{ij}$是样本i属于类别j的one-hot标签，$p_{ij}$是模型预测样本i属于类别j的概率。公式中的第一个求和符号遍历所有样本，第二个求和符号遍历所有类别。	

分类交叉熵的含义是用来评估模型预测结果与真实标签之间的差异。**当模型预测结果和真实标签一致时，交叉熵为0**；当它们之间的差距越大时，交叉熵的值越大。因此，分类交叉熵可以作为模型的损失函数，用来优化模型的参数。

在实际训练中，分类交叉熵通常与反向传播算法结合使用，以计算每个参数的梯度并更新参数。可以使用基于梯度下降的优化算法，如随机梯度下降（SGD）或Adam来最小化分类交叉熵损失，以提高模型的分类精度。

### 稀疏分类交叉熵

稀疏分类交叉熵和普通的分类交叉熵在计算损失函数的方式上有所不同。

在普通的分类交叉熵中，目标值（即真实标签）采用了one-hot编码的方式，即只有真实类别对应的那个元素为1，其余元素都为0。在计算损失函数时，需要将神经网络输出的概率分布和目标值进行对比，计算交叉熵损失。

而在稀疏分类交叉熵中，**目标值采用的是整数形式的标签，即每个样本的真实标签对应一个整数值**。这种方式避免了使用one-hot编码所产生的大量零元素，节省了存储空间。在计算损失函数时，需要将神经网络输出的概率分布和目标值进行对比，计算交叉熵损失。

因此，稀疏分类交叉熵和普通的分类交叉熵的主要区别在于目标值的表示方式不同，稀疏分类交叉熵适合处理类别数目较多的情况，能够更有效地处理稀疏数据。

> 假设有一个图像分类问题，共有10个类别。使用稠密编码（dense encoding），每个样本的标签是一个长度为10的向量，每个元素代表一个类别，值为1表示属于该类别，值为0表示不属于。比如样本1属于第5个类别，则其标签为[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]。
>
> 使用稀疏编码（sparse encoding），每个样本的标签是一个标量，表示该样本所属的类别。**比如样本1属于第5个类别，则其标签为4（从0开始计数）**。在训练模型时，使用稀疏编码可以节省存储空间和计算资源。
>
> 对于稠密编码，其分类交叉熵的计算方式为：
>
> $-\sum_{i=1}^{10} y_i \log(\hat{y_i})$
>
> 其中，$y_i$是样本的真实标签中第$i$个元素的值，$\hat{y_i}$是模型预测的该元素的概率值。
>
> 对于稀疏编码，其分类交叉熵的计算方式为：
>
> $-\log(\hat{y_j})$
>
> 其中，$j$是样本的真实标签。由于**真实标签只有一个元素为1，因此只需要计算该元素对应的预测概率值的对数即可**。



## Focal Loss

Focal Loss（焦点损失）是由Lin et al.在2017年的论文《Focal Loss for Dense Object Detection》中提出的一种损失函数，主要用于解决目标检测任务中的类别不平衡问题。**专门用于解决分类问题中不平衡数据集的训练问题**。它通过调整难易样本的权重来解决模型在错误分类方面的问题。Focal Loss的主要思想是**减少易分类样本的权重**，使模型更加关注困难样本。

Focal Loss引入两个参数：调节因子（**调节困难样本的重要性**）和焦点参数（**控制调节因子的程度**）。通过增加焦点参数，可以进一步减少易分类样本的权重，使模型更加关注困难样本。通过这种方式，Focal Loss有助于提高模型在罕见类别上的性能。

假设我们有一个二分类问题，样本分为正样本（Positive）和负样本（Negative）。传统的交叉熵损失函数对于类别不平衡的问题表现不佳，因为它倾向于优化常见类别，而忽视罕见类别。Focal Loss通过引入调节因子和焦点参数，使得模型更关注困难样本，以此来解决类别不平衡问题。

首先，假设$p_t$表示样本属于正样本的概率，$p_t \in [0, 1]$。那么样本属于负样本的概率可以表示为$1 - p_t$。经过逻辑回归（sigmoid）函数处理后，我们可以得到预测概率：

$$
\hat{y}_t =
\begin{cases}
p_t, & \text{if the ground truth label is positive} \\
1 - p_t, & \text{otherwise}
\end{cases}
$$

接下来，我们定义调节因子$(1-\hat{y}_t)^\gamma$，其中$\gamma \geq 0$。这个调节因子用于降低容易分类的样本的权重，使得模型更加关注困难样本。当$\gamma=0$时，调节因子为常数，即不对样本进行加权。当$\gamma>0$时，调节因子会随着预测概率的增加而减小。

将上述两个部分相乘，并使用交叉熵损失函数计算每个样本的损失。整个Focal Loss的公式如下：

$$\text{FL}(p_t) = -\alpha_t (1-\hat{y}_t)^\gamma \log(\hat{y}_t)$$

其中：

- $\alpha_t$是一个平衡因子，用于调节正负样本之间的权重关系。一般情况下，$\alpha_t$可以根据**类别频率**进行设置，以降低常见类别的权重。

- $(1-\hat{y}_t)^\gamma$是调节因子，它降低了容易分类的样本的权重，使得模型更关注困难样本。

  > 由于$1-\hat{y}_t$小于1，所以与其相乘会变小。
  >
  > 当分类$\hat{y}_t$接近1时，$1-\hat{y}_t$ 对应趋于0，此时调节因子的焦点参数$\gamma$越大，$1-\hat{y}_t$越小即Loss值越小，即模型分类的越确定损失越小，相反当模型分类不确定时调节因子缩小程度不大。

- $\log(\hat{y}_t)$表示预测概率的对数。

通过最小化所有样本的Focal Loss，我们可以训练出在类别不平衡问题上表现更好的模型。

需要注意的是，以上是Focal Loss的基本原理和推导过程。具体应用中，可能还会对公式进行微调或引入其他参数来适应具体任务的需求。

## CTC

**CTC全称：Connectionist temporal classification， 主要用于处理序列标注问题中的输入与输出标签的对齐问题。**

参考文章：
https://www.cnblogs.com/shiyublog/p/10493348.html#_label0

3. 

这些资源可以帮助你深入理解Gumbel-Softmax算法的原理和应用。

# 对比损失函数

## Contrastive Loss

**对比损失**在**非监督学习**中应用很广泛。最早源于2006年Yann LeCun的”Dimensionality Reduction by Learning an Invariant Mapping“，该损失函数主要是用于降维中，即本来相似的样本，在经过降维（特征提取）后，在特征空间中，两个样本仍旧相似；而原本不相似的样本，在经过降维后，在特征空间中，两个样本仍旧不相似。同样，该损失函数也可以很好的表达**成对样本的匹配程度**。

在非监督学习时，对于一个数据集内的所有样本，因为我们没有样本真实标签，所以在对比学习框架下，通常以每张图片作为单独的语义类别，并假设：同一个图片做不同变换后不改变其语义类别，比如一张猫的图片，旋转或局部图片都不能改变其猫的特性。

因此，假设对于原始图片X，分别对其做不同变换得到A和B,此时对比损失希望A、B之间的特征距离要小于A和任意图片Y的特征距离。

定义对比损失函数L为：

L(W,(Y, $ X_ {1} $ , $ X_ {2} $ ))= $ \frac {1}{N} $ $ \sum _ {n=1}^ {N} $ $( YD_ {W}^ {2} $ +(1-Y) $ \max $ $ (M-D_ {w},0)^ {2} )$ 

其中, $ D_ {W} $ ( $ X_ {1} $ , $ X_ {2} $ )= $ ||X_ {1} $ - $ X_ {2} $ $ ||_ {2} $ =( $ \sum _ {i=1}^ {P} $ $ (X_ {1}^ {i}-X_ {2}^ {i})^ {2} $ ) $ ^ {\frac {1}{2}} $ 

Dw代表两个样本特征的欧式距离,P代表特征的维度,Y为两个样本是否匹配的标签(Y=1代表两个样本相似或匹配, Y=0代表两个样本不相似或不匹配),M为设定的减值(距离超过M的把其loss看作0, 即如果两个不相似特征离得很远, 那么对比loss应该是很低的) ,N为样本数量。

通过损失函数可以发现,对比损失可以很好的描述成对样本的匹配程度, 用于训练提取特征的模型:

当Y=1时,即两个样本相似或匹配时,损失函数 $ L_ {s} $ = $ \frac {1}{N} $ $ \sum _ {n=1}^ {l} $ $ YD_ {W}^ {2} $ ,即如果原本相似或匹配的样本如果其被模型提取的特征欧氏距离很大, 说明模型效果不好导致loss很大。

当Y=0时,即两个样本不相似或不匹配时,损失函数 $ L_ {s} $ =(1-Y) $ \max $ $ (m-D_ {w},0)^ {2} $ ,如果这时两个样本被模型提取的特征欧式距离很小, 那么loss会变大以增大模型的惩罚从而使loss减小, 如果两个样本被模型提取的特征欧式距离很大, 说明两个样本特征离得很远, 此时如果超过减值M则把其loss看作0, 此时的loss很小。

除此之外，对比损失还有很多其他的数学公式表示形式。

参考文章：
https://zhuanlan.zhihu.com/p/590547670
